# Ход выполнения лабораторной работы
1. Загрузка данных
2. Предобработка данных
3. Логистическая регрессия + TF-IDF
4. Создать токинизатор bert и преобразовать им датасет
5. Провести fine-tuning bert на датасете
6. Оценка результатов

# Ответы на вопросы
1. Зачем нужен positional encoding?

Механизм self-attention по своей природе инвариантен к порядку токенов: если переставить слова в предложении, матрица внимания изменится, но модель не "заметит" этого, если не добавить дополнительную информацию о позиции.
Фактически модель не различит "кошка видит собаку" и "собака видит кошку"

2. В чём отличие encoder-only и decoder-only моделей?
* Encoder-only нужны для извлечения признаков из текста. Задачи: классификация, извлечение именнованых сущностей, семантический поиск, анализ тональности и т.д.
* Decoder-only нужны для генерации текста. Задачи: все связанное с генерацией текста.
3. Почему трансформеры лучше RNN в NLP?
* Основное преимущество в том, что трансформеры могут обрабатывать текст паралелльно, что сильно снижает время обучения и позволяет масштабировать модели.
* Сохранение контекста, трансформеры способны моделировать длинные зависимости

# Источники
1. https://huggingface.co/papers/1810.04805
2. https://huggingface.co/docs/transformers/model_doc/bert#notes
3. https://habr.com/ru/companies/otus/articles/702838/
4. https://aws.amazon.com/ru/what-is/transformers-in-artificial-intelligence/